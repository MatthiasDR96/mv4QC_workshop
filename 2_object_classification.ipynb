{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: object classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will determine the specie of fish based on an image of that fish. We will use Deep Learning classification to do the job. First, we will import all necessary modules that we need to get the job done. The next parts of the code are organized in: data analysis, data preprocessing, model training, and model validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os # Module to access Operating System\n",
    "import keras # Module for deep learning\n",
    "!pip -q install keras-cv > /dev/null\n",
    "import keras_cv # Keras module for object detection\n",
    "import numpy as np # Module for matrix operations and linear algebra\n",
    "import seaborn as sns # Module for plotting\n",
    "import tensorflow as tf # Module for deep learning\n",
    "from keras import backend as K # Module for tensor operations\n",
    "import matplotlib.pyplot as plt # Module for plotting\n",
    "from sklearn.metrics import confusion_matrix # Module for model evaluation\n",
    "from keras.applications.resnet50 import ResNet50 # Pretrained ResNet50 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first step, we will download the data. The data will be stored in several folders per class. Each class folder contains images of the corresponsing class. Next, we define a Dataloader for the train and testset that allow the network to retrieve the dataset in batches. We use a batch size of 9. Next, we split the validation dataset in a validation and test set. We also visualize some samples from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "if not os.path.exists('data/fish_data_classification'):\n",
    "    !wget -q --no-check-certificate -O dataset.zip \"https://kuleuven-my.sharepoint.com/:u:/g/personal/matthias_deryck_kuleuven_be/Ed2e5ehats5Ery2K15Gzos0B6AlAOKnBoA8FMTRm3Xmkrw?e=2G6qWx&download=1\"\n",
    "    !unzip -q dataset.zip -d data\n",
    "    !rm dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class list\n",
    "class_list = ['ANF', 'BIB', 'GUU', 'PLE', 'SOL', 'WIT']\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 9\n",
    "\n",
    "# Get Dataloader for the training set (70%)\n",
    "train_ds = keras.utils.image_dataset_from_directory(\"./data/fish_data_classification/\", validation_split=0.3, subset=\"training\", seed=1337, image_size=(150, 150), batch_size=batch_size)\n",
    "\n",
    "# Get Dataloader for the validation set (30%)\n",
    "val_ds = keras.utils.image_dataset_from_directory(\"./data/fish_data_classification/\", validation_split=0.3, subset=\"validation\", seed=1337, image_size=(150, 150), batch_size=batch_size)\n",
    "\n",
    "# Split validation set further in 15% validation and 15% testset\n",
    "val_batches = tf.data.experimental.cardinality(val_ds)\n",
    "test_ds = val_ds.take((val_batches) // 2)\n",
    "val_ds = val_ds.skip((val_batches) // 2)\n",
    "\n",
    "# Print results\n",
    "print(f'\\nThe validation set contains {len(val_ds)*batch_size} samples')\n",
    "print(f'\\nThe test set contains {len(test_ds)*batch_size} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot one batch in grid\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(np.array(images[i]).astype(\"uint8\"))\n",
    "        plt.title(class_list[int(labels[i])])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have limited data, data augmentation can help improving the model performance. In this case, we perform a random horizontal flip of the image, and a random rotation of the image with a probability of 10%. We apply the data augmentation to the train images and visualize some augmented images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data augmentation layers\n",
    "data_augmentation_layers = [\n",
    "    keras.layers.RandomFlip(\"horizontal\"),\n",
    "    keras.layers.RandomRotation(0.1),\n",
    "]\n",
    "\n",
    "# Define data augmentation function\n",
    "def data_augmentation(images):\n",
    "    for layer in data_augmentation_layers:\n",
    "        images = layer(images)\n",
    "    return images\n",
    "\n",
    "# Apply data_augmentation to the training images.\n",
    "train_ds = train_ds.map(lambda img, label: (data_augmentation(img), label), num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot one batch in grid\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        augmented_images = data_augmentation(images)\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(np.array(augmented_images[0]).astype(\"uint8\"))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a full and efficient usage of the GPU and CPU, we apply the following methods to each Dataloader that take care of this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefetching samples in GPU memory helps maximize GPU utilization.\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we will train our model. The model that we use is a pretrained ResNet50 model that was trained on ImageNet. We only use the feature extraction part of this model, not the classification part, this we will define ourself. Therefore, we make a new model with the feature extraction layers and a new fully connected output layer. We finetune the full model, thus the feature extraction part, and the classification part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ResNet50 model with pre-trained weights\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
    "\n",
    "# Set the parameters of the feature extraction part as also trainable\n",
    "base_model.trainable = True\n",
    "\n",
    "# Get the output layer of the base model (the output of the feature extraction part)\n",
    "x = base_model.output\n",
    "\n",
    "# Convert the features of shape `base_model.output_shape[1:]` to vectors\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Apply a dense classifier with six output units (we have six classes)\n",
    "outputs = keras.layers.Dense(6, activation='softmax')(x)\n",
    "\n",
    "# Define final model\n",
    "model = keras.Model(base_model.input, outputs)\n",
    "\n",
    "# Summarize final model\n",
    "#model.summary(show_trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the model, we can compile it with the proper optimizer, loss function, and evaluation metrics. After that we can train it for the specified number of epochs. First, we define some functions that will calculate some interesting evaluation metrics after each epoch: recall, precision, and f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the average recall\n",
    "def recall(y_true, y_pred):\n",
    "    y_true = K.ones_like(y_true)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    all_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (all_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "# Function to compute the average precision\n",
    "def precision(y_true, y_pred):\n",
    "    y_true = K.ones_like(y_true)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "# Function to compute average F1 score\n",
    "def f1_score(y_true, y_pred):\n",
    "    precision_ = precision(y_true, y_pred)\n",
    "    recall_ = recall(y_true, y_pred)\n",
    "    return 2*((precision_*recall_)/(precision_+recall_+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of epochs to train\n",
    "epochs = 30\n",
    "\n",
    "# Set optimizer, loss function, and evaluation metrics\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.SGD(), # Stochastic gradient descent optimizer\n",
    "    loss=[keras.losses.SparseCategoricalCrossentropy()], # Crossentropy loss for multi-class classification\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy(), f1_score, precision, recall]) # Crossentropy accuracy for multi-class classification\n",
    "\n",
    "# Train model\n",
    "hist = model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, the model was evaluated on the training and the validation set. It is important to also evaluate the model on a set of images that it has never seen before. So we evaluate it again on the test set and we also compute the confusion matrix. Lastly, we loop over several images in the dataset and predict the class as if we would do in production. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "result = model.evaluate(test_ds)\n",
    "\n",
    "# Print result\n",
    "print(f'\\nThe model has an accuracy of {result[1]*100}% on the test set')\n",
    "print(f'\\nThe model has an precision of {result[3]*100}% on the test set')\n",
    "print(f'\\nThe model has an recall of {result[4]*100}% on the test set')\n",
    "print(f'\\nThe model has an F1-score of {result[2]*100}% on the test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load batch and save predictions for confusion matrix\n",
    "y_pred = []\n",
    "y_true = []\n",
    "for images, labels in test_ds:\n",
    "\ty_pred += list(np.argmax(model.predict(images), axis = 1))\n",
    "\ty_true += list(labels.numpy())\n",
    "\n",
    "# Plot confusion matrix\n",
    "ax= plt.subplot()\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='g', ax=ax)\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels')\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.xaxis.set_ticklabels(class_list)\n",
    "ax.yaxis.set_ticklabels(class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all folders and predict the class for the first image\n",
    "for specie in class_list:\n",
    "\n",
    "    # Define image path of the first image in the folder\n",
    "    image_path = \"./data/fish_data_classification/\" + specie + \"/\" + os.listdir(\"./data/fish_data_classification/\" + specie)[0]\n",
    "                                                    \n",
    "    # Load image\n",
    "    img = keras.utils.load_img(image_path, target_size=(150, 150))\n",
    "\n",
    "    # Convert image to array\n",
    "    input_arr = np.array([keras.utils.img_to_array(img)])\n",
    "\n",
    "    # Make predictions (outputs probabilities for each function)\n",
    "    predictions = model.predict(input_arr, verbose=0)\n",
    "\n",
    "    # Get the class with the highest probability\n",
    "    predicted_class = class_list[predictions.argmax()]\n",
    "\n",
    "    # Print result\n",
    "    print(f\"\\nThe model predicts {predicted_class} and the ground truth class is {specie}\")\n",
    "\n",
    "    # Show image\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
