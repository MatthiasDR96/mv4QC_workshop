{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: object classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os # Module to access Operating System\n",
    "import keras # Module for deep learning\n",
    "!pip -q install keras-cv > /dev/null\n",
    "import keras_cv # Keras module for object detection\n",
    "import numpy as np # Module for matrix operations and linear algebra\n",
    "import seaborn as sns # Module for plotting\n",
    "import tensorflow as tf # Module for deep learning\n",
    "from keras import backend as K # Module for tensor operations\n",
    "import matplotlib.pyplot as plt # Module for plotting\n",
    "from sklearn.metrics import confusion_matrix # Module for model evaluation\n",
    "from keras.applications.resnet50 import ResNet50 # Pretrained ResNet50 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first step, we will download the data and split the total dataset in a train, validation, and test set. We will also visualize some samples from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "if not os.path.exists('data/fish_data_classification'):\n",
    "    !wget -q --no-check-certificate -O dataset.zip \"https://kuleuven-my.sharepoint.com/:u:/g/personal/matthias_deryck_kuleuven_be/Ed2e5ehats5Ery2K15Gzos0B6AlAOKnBoA8FMTRm3Xmkrw?e=2G6qWx&download=1\"\n",
    "    !unzip -q dataset.zip -d data\n",
    "    !rm dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class list\n",
    "class_list = ['ANF', 'BIB', 'GUU', 'PLE', 'SOL', 'WIT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training set (70%)\n",
    "train_ds = keras.utils.image_dataset_from_directory(\"./data/fish_data_classification/\", validation_split=0.3, subset=\"training\", seed=1337, image_size=(150, 150), batch_size=9)\n",
    "\n",
    "# Get validation set (30%)\n",
    "val_ds = keras.utils.image_dataset_from_directory(\"./data/fish_data_classification/\", validation_split=0.3, subset=\"validation\", seed=1337, image_size=(150, 150), batch_size=9)\n",
    "\n",
    "# Split validation set further in 15% validation and 15% testset\n",
    "val_batches = tf.data.experimental.cardinality(val_ds)\n",
    "test_ds = val_ds.take((val_batches) // 2)\n",
    "val_ds = val_ds.skip((val_batches) // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot one batch in grid\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(np.array(images[i]).astype(\"uint8\"))\n",
    "        plt.title(class_list[int(labels[i])])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have limited data, augmentation can help improving the model performance. In this case, we perform a random horizontal flip of the image, and a random rotation of the image with a probability of 10%. We visualize some augmented images and apply the data augmentation to the train images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data augmentation layers\n",
    "data_augmentation_layers = [\n",
    "    keras.layers.RandomFlip(\"horizontal\"),\n",
    "    keras.layers.RandomRotation(0.1),\n",
    "]\n",
    "\n",
    "# Define data augmentation function\n",
    "def data_augmentation(images):\n",
    "    for layer in data_augmentation_layers:\n",
    "        images = layer(images)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot one batch in grid\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        augmented_images = data_augmentation(images)\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(np.array(augmented_images[0]).astype(\"uint8\"))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply data_augmentation to the training images.\n",
    "train_ds = train_ds.map(lambda img, label: (data_augmentation(img), label), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Prefetching samples in GPU memory helps maximize GPU utilization.\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we will train our model. The model that we use is a pretrained ResNet50 model that was trained on ImageNet. We want to finetune the model and thus also modify the parameters of the feature extraction layers. We make a new model with the feature extraction layers and a new fully connected output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ResNet50 model with pre-trained weights\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
    "\n",
    "# Set trainable for finetuning\n",
    "base_model.trainable = True\n",
    "\n",
    "# Get the output layer of the base model\n",
    "x = base_model.output\n",
    "\n",
    "# Convert features of shape `base_model.output_shape[1:]` to vectors\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "# A Dense classifier with six units (six classes)\n",
    "outputs = keras.layers.Dense(6, activation='softmax')(x)\n",
    "\n",
    "# Define final model\n",
    "model = keras.Model(base_model.input, outputs)\n",
    "\n",
    "# Summarize final model\n",
    "#model.summary(show_trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the model, we can compile it with the proper optimizer, loss function, and evaluation metrics. After that we can train it for e specified number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the average recall\n",
    "def recall(y_true, y_pred):\n",
    "    y_true = K.ones_like(y_true)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    all_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (all_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "# Function to compute the average precision\n",
    "def precision(y_true, y_pred):\n",
    "    y_true = K.ones_like(y_true)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "# Function to compute average F1 score\n",
    "def f1_score(y_true, y_pred):\n",
    "    precision_ = precision(y_true, y_pred)\n",
    "    recall_ = recall(y_true, y_pred)\n",
    "    return 2*((precision_*recall_)/(precision_+recall_+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of epochs to train\n",
    "epochs = 30\n",
    "\n",
    "# Set optimizer, loss function, and accuracy metrics\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.SGD(), # Stochastic gradient descent optimizer\n",
    "    loss=[keras.losses.SparseCategoricalCrossentropy()], # Crossentropy loss for multi-class classification\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy(), f1_score, precision, recall]) # Crossentropy accuracy for multi-class classification\n",
    "\n",
    "# Train model\n",
    "hist = model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, the model was evaluated on the training and the validation set. It is important to also evaluate the model on a set of images that it has never seen before. So we evaluate it again on the test set. Lastly, we loop over several images in the dataset and predict the class as if we would do in production. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "result = model.evaluate(test_ds)\n",
    "\n",
    "# Print result\n",
    "print(f'\\nThe model has an accuracy of {result[1]*100}% on the test set')\n",
    "print(f'\\nThe model has an precision of {result[3]*100}% on the test set')\n",
    "print(f'\\nThe model has an recall of {result[4]*100}% on the test set')\n",
    "print(f'\\nThe model has an F1-score of {result[2]*100}% on the test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load batch and save predictions\n",
    "y_pred = []\n",
    "y_true = []\n",
    "for images, labels in test_ds:\n",
    "\ty_pred += list(np.argmax(model.predict(images), axis = 1))\n",
    "\ty_true += list(labels.numpy())\n",
    "\n",
    "# Plot confusion matrix\n",
    "ax= plt.subplot()\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='g', ax=ax)\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels')\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.xaxis.set_ticklabels(class_list)\n",
    "ax.yaxis.set_ticklabels(class_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all folders and predict the class for the first image\n",
    "for specie in class_list:\n",
    "\n",
    "    # Define image path of the first image in the folder\n",
    "    image_path = \"./data/fish_data_classification/\" + specie + \"/\" + os.listdir(\"./data/fish_data_classification/\" + specie)[0]\n",
    "                                                    \n",
    "    # Load image\n",
    "    img = keras.utils.load_img(image_path, target_size=(150, 150))\n",
    "\n",
    "    # Convert image to array\n",
    "    input_arr = keras.utils.img_to_array(img)\n",
    "    input_arr = np.array([input_arr])  \n",
    "\n",
    "    # Make predictions (outputs probabilities for each function)\n",
    "    predictions = model.predict(input_arr, verbose=0)\n",
    "\n",
    "    # Get the class with the highest probability\n",
    "    predicted_class = class_list[predictions.argmax()]\n",
    "\n",
    "    # Print result\n",
    "    print(f\"\\nThe model predicts {predicted_class} and the ground truth class is {specie}\")\n",
    "\n",
    "    # Show image\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
