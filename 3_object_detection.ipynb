{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Object detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will locate and classify fish in images. We will use Deep Learning object detection to do the job. First, we will import all necessary modules that we need to get the job done. The next parts of the code are organized in: data analysis, data preprocessing, model training, and model validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "%load_ext tensorboard\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import os # Module to access Operating System\n",
    "import cv2 # Module for coputer vision\n",
    "import keras # Module for deep learning\n",
    "!pip -q install keras-cv > /dev/null\n",
    "import keras_cv # Keras module for object detection\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"  # @param [\"tensorflow\", \"jax\", \"torch\"]\n",
    "from keras_cv import bounding_box\n",
    "from keras_cv import visualization\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a first step we will load all our data. The data will be stored in a folder 'fish_data_detection' with subfolders 'images' and 'labels'. The images contain all images in the dataset, and the labels contain all corresponding labels in YOLO format. This format requires a .txt file for every image with a row for every object in the image that contains the class label (an integer) and the bounding box label (in the format [x, y, witdh height] of the bounding box)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "if not os.path.exists('data/fish_data_detection'):\n",
    "    !wget -q --no-check-certificate -O dataset.zip \"https://kuleuven-my.sharepoint.com/:u:/g/personal/matthias_deryck_kuleuven_be/EQwO_pgG8vZIk7ZeuO-gUFMB4awxHzEHSps64pORv6-IqA?e=ia37Dl&download=1\"\n",
    "    !wget -q --no-check-certificate -O model.h5 \"https://kuleuven-my.sharepoint.com/:u:/g/personal/matthias_deryck_kuleuven_be/EbjLZe0c8ORLsx9xFlgnrhQBkcSIVrykfv8508wVvfTCBw?e=3Uvgp2&download=1\"\n",
    "    !unzip -q dataset.zip -d data\n",
    "    !rm dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class mapping\n",
    "class_ids =  ['ANF', 'BIB', 'GUR', 'GUU', 'HAD', 'MEG', 'PLE', 'SOL', 'WHG', 'WIT']\n",
    "class_mapping = dict(zip(range(len(class_ids)), class_ids))\n",
    " \n",
    "# Path to images and annotations\n",
    "path_images = \"./data/fish_data_detection/images/\"\n",
    "path_annot = \"./data/fish_data_detection/labels/\"\n",
    "\n",
    "# Print results\n",
    "print(f'The dataset contains {len(os.listdir(path_images))} samples.')\n",
    "print(f'\\nThe classes are mapped to integers following: {class_mapping}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we will iterate over every image and save the path to the image, the bounding box coordinates, and the class labels in separate lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init lists\n",
    "image_paths = []\n",
    "bbox = []\n",
    "classes = []\n",
    "\n",
    "# Loop over all samples\n",
    "for image_path in os.listdir(path_images):\n",
    "\n",
    "    # Read image\n",
    "    image = cv2.imread('./data/fish_data_detection/images/' + image_path)\n",
    "    h, w, c = image.shape\n",
    "\n",
    "    # Load labels\n",
    "    boxes = []\n",
    "    class_ids = []\n",
    "    with open(path_annot + image_path.replace('.jpg', '.txt'), 'r') as f:\n",
    "        for line in f:\n",
    "            tmp = [float(x) for x in line.strip().split()]\n",
    "            boxes.append([tmp[1]*w, tmp[2]*h, tmp[3]*w, tmp[4]*h])\n",
    "            class_ids.append(tmp[0])\n",
    "\n",
    "    # Add to lists\n",
    "    image_paths.append(path_images + image_path)\n",
    "    bbox.append(boxes)\n",
    "    classes.append(class_ids)\n",
    "\n",
    "# Print results\n",
    "print(f'The first image has the following bounding box data: {bbox[0]}')\n",
    "print(f'\\nThe first image has the following class label data: {classes[0]}, which means it it of the class ANF')\n",
    "\n",
    "# Compress the data for easy training\n",
    "bbox = tf.ragged.constant(bbox)\n",
    "classes = tf.ragged.constant(classes)\n",
    "image_paths = tf.ragged.constant(image_paths)\n",
    "\n",
    "# Convert the data to a dataset object suitable for Keras\n",
    "data = tf.data.Dataset.from_tensor_slices((image_paths, classes, bbox))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will performa a train and validation split of the data. We will have 70% training data, and 30% validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of validation samples\n",
    "SPLIT_RATIO = 0.3\n",
    "num_val = int(len(os.listdir(path_images)) * SPLIT_RATIO)\n",
    " \n",
    "# Split the dataset into train (70%) and validation sets (30%)\n",
    "val_data = data.take(num_val)\n",
    "train_data = data.skip(num_val)\n",
    "\n",
    "# Print results\n",
    "print(f'The training set contains {len(train_data)} samples')\n",
    "print(f'\\nThe validation set contains {len(val_data)} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will wrap a Dataloader around the dataset that makes it possible for the network to retrieve batches of the dataset in the proper format. The training loop of Keras requires the samples to be formatted in a dictionary with keys \"images\", and \"bounding_boxes\", where the latter has keys 'classes', and 'boxes'. We define a batch size of 8, which means that the network retrieves samples in batches of 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that converts the data in Keras format\n",
    "def load_dataset(image_path, classes, bbox):\n",
    "\n",
    "    # Read the image and decode it to Keras format\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "\n",
    "    # Format the labels\n",
    "    bounding_boxes = {\n",
    "        \"classes\": tf.cast(classes, dtype=tf.float32),\n",
    "        \"boxes\": bbox,\n",
    "    }\n",
    "\n",
    "    return {\"images\": tf.cast(image, tf.float32), \"bounding_boxes\": bounding_boxes}\n",
    " \n",
    "# Create a data loader for the training and validation set\n",
    "train_ds = train_data.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds = val_data.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Define the batch size\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Construct a Ragged dataset that can be loaded in batches\n",
    "train_ds = train_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_ds = val_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "# Shuffle training set\n",
    "train_ds = train_ds.shuffle(BATCH_SIZE * 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For bebugging purposes, we can validate visually if everything is correct by visualizing a batch of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize dataste samples in a grid\n",
    "def visualize_dataset(inputs, value_range, rows, cols, bounding_box_format):\n",
    "    inputs = next(iter(inputs.take(1)))\n",
    "    images, bounding_boxes = inputs[\"images\"], inputs[\"bounding_boxes\"]\n",
    "    visualization.plot_bounding_box_gallery(\n",
    "        images,\n",
    "        value_range=value_range,\n",
    "        rows=rows,\n",
    "        cols=cols,\n",
    "        y_true=bounding_boxes,\n",
    "        scale=5,\n",
    "        font_scale=0.7,\n",
    "        bounding_box_format=bounding_box_format,\n",
    "        class_mapping=class_mapping,\n",
    "    )\n",
    " \n",
    "# Visualize one batch of the training set\n",
    "visualize_dataset(train_ds, bounding_box_format=\"center_xywh\", value_range=(0, 255), rows=2, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the labels are correctly placed in the images. A next step is data augementation which will apply several image transformations to our data in order to enhance the dataset and obtain a more robust model. We define some augmentation layers and apply them to the training set. For the validation set, we only perform resizing so that the image that gets fed to the network is in the proper size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define augmentation layers for training set\n",
    "augmenter = keras.Sequential(\n",
    "    layers=[\n",
    "        keras_cv.layers.RandomFlip(mode=\"horizontal\", bounding_box_format=\"center_xywh\"),\n",
    "        keras_cv.layers.JitteredResize(\n",
    "            target_size=(640, 640),\n",
    "            scale_factor=(1.0, 1.0),\n",
    "            bounding_box_format=\"center_xywh\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define resizing for validation set\n",
    "resizing = keras_cv.layers.JitteredResize(\n",
    "    target_size=(640, 640),\n",
    "    scale_factor=(1.0, 1.0),\n",
    "    bounding_box_format=\"center_xywh\",\n",
    ")\n",
    "\n",
    "# Define a mapping to ['images'] and '[bounding_boxes']\n",
    "def dict_to_tuple(inputs):\n",
    "    return inputs[\"images\"], bounding_box.to_dense(inputs[\"bounding_boxes\"], max_boxes=32)\n",
    "\n",
    "# Apply data augmentation to training set\n",
    "train_ds = train_ds.map(augmenter, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Apply resizing to validation set\n",
    "val_ds = val_ds.map(resizing, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a full and efficient usage of the GPU and CPU, we apply the following methods to each Dataloader that take care of this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefetching samples in GPU memory helps maximize GPU utilization.\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following step, we will define a pretrained YOLOv8 model that we will train with our custom data. We use a pretrained backbone (the underlying architecture) and wrap this in a detector of which we can set the custom number of classes we want to detect. We compile the model with the proper optimizer, loss function for classification, and loss function for box regression. Lastly, we define some callbacks that will be called after every epoch, one that calculates COCO evaluation metrics, one that saves the best model, and one that logs data for visualization in Tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model using a pretrained backbone\n",
    "backbone = keras_cv.models.YOLOV8Backbone.from_preset(\n",
    "    \"yolo_v8_m_backbone_coco\"  # We will use yolov8 medium backbone with coco weights\n",
    ")\n",
    "\n",
    "# Build a yolov8 model using the feature extractor backbone\n",
    "model = keras_cv.models.YOLOV8Detector(\n",
    "    num_classes=len(class_mapping),\n",
    "    bounding_box_format=\"center_xywh\",\n",
    "    backbone=backbone,\n",
    "    fpn_depth=1,\n",
    ")\n",
    "\n",
    "# Set optimizer, loss function, and accuracy metrics\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001, global_clipnorm=10.0),\n",
    "    classification_loss=\"binary_crossentropy\",\n",
    "    box_loss=\"ciou\",\n",
    ")\n",
    "\n",
    "# Define COCO metrics callback\n",
    "coco_metrics_callback = keras_cv.callbacks.PyCOCOCallback(\n",
    "    val_ds, bounding_box_format=\"center_xywh\"\n",
    ")\n",
    "\n",
    "# Define a callback that saves the best model\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    'model.h5',\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode=\"auto\",\n",
    "    save_freq=\"epoch\",\n",
    "    initial_value_threshold=None,\n",
    ")\n",
    "\n",
    "# Define Tensorboard callback\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After compilation of the model, we can train it given our dataset. We will train for one epoch as training for more epochs takes a lot of time. We trained this network previously for 30 epochs and saved the model that we will load hereafter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Tensorboard that will read the logs made by the model callback and visualize it in graphs.\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=1,\n",
    "    callbacks=[coco_metrics_callback, tensorboard_callback, model_checkpoint_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After model training, we will validate it. First, we load the trained model. We define a NonMax supressor that puts a threshold on the detection probabilities. Only detections with a probability higher than the threshold will be shown. Second, we visualize some detections on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best saved model\n",
    "model.load_weights('model.h5')\n",
    "\n",
    "# Define Nonmax supressor\n",
    "model.prediction_decoder = keras_cv.layers.NonMaxSuppression(\n",
    "    bounding_box_format=\"center_xywh\",\n",
    "    from_logits=True,\n",
    "    iou_threshold=0.2,\n",
    "    confidence_threshold=0.55,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize detections\n",
    "def visualize_detections(model, dataset, bounding_box_format):\n",
    "\n",
    "    # Read one batch\n",
    "    images, y_true = next(iter(dataset.take(1)))\n",
    "\n",
    "    # Predict\n",
    "    y_pred = model.predict(images)\n",
    "\n",
    "    # Plot ground thruth and predictions\n",
    "    visualization.plot_bounding_box_gallery(\n",
    "        images,\n",
    "        value_range=(0, 255),\n",
    "        bounding_box_format=bounding_box_format,\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        scale=4,\n",
    "        rows=2,\n",
    "        cols=2,\n",
    "        show=True,\n",
    "        font_scale=0.7,\n",
    "        class_mapping=class_mapping,\n",
    "    )\n",
    "\n",
    "# Visualize detections\n",
    "visualize_detections(model, dataset=val_ds, bounding_box_format=\"center_xywh\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
